{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-Parallel to Distributed Data-Parallel\n",
    "\n",
    "In this session, we're going to try and bridge the gap between data parallelism in the shared memory case, which is what we'd learned in the parallel programming course and distributed data parallelism.\n",
    "So taking that idea of data parallelism and extending that to the situation where you no longer have data on just one node anymore. Now you have data spread across several independent nodes. \n",
    "\n",
    "Scala's Parallel Collections is a collections abstraction over shared memory data-parallel execution.\n",
    "\n",
    "Shared memory data parallelism: \n",
    "\n",
    "- Split the data on same machine. \n",
    "- Workers/threads independently operate on the data shards in parallel. \n",
    "- Combine when done (if necessary).\n",
    "\n",
    "Distributed Data-Parallelism\n",
    "\n",
    "- Split the data over several nodes. \n",
    "- Nodes independently operate on the data shards in parallel. \n",
    "- Combine when done ( if necessary).\n",
    "\n",
    "We have to worry about network latency because of data sharing and communication between nodes.\n",
    "\n",
    "However, like parallel collections, we can keep  collections  abstraction over distributed data-parallel execution.\n",
    "\n",
    "> **Shared  memory case**: \n",
    " Data-parallel programming model. Data partitioned in memory and operated upon in parallel. \n",
    "\n",
    "> **Distributed case**:  \n",
    "Data-parallel programming model. Data partitioned between machines, network in between, operated upon in parallel. \n",
    "\n",
    "Overall, most all properties we learned about related to shared memory data-parallel collections can be applied to their distributed counterparts. \n",
    "However, must now consider latency when using our model. \n",
    "\n",
    "Throughout this part of the course we will use the Apache Spark framework for distributed data-parallel programming. \n",
    "\n",
    "Spark implements a distributed data parallel model called Resilient Distributed Datasets (RDDs)\n",
    "\n",
    "RDD are distributed counterparts of parallel collections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Data-Parallel: High Level Illustration \n",
    "Given a large datset (say Wikiepdia English 48.4GB) that can't fit into memory of single node. SPark will chunk up the data using some distribution mechanism and distribute it over cluster of machines.\n",
    "\n",
    "From there think of your distributed data like single collection. Spark will return a reference to entire distributed datasets.\n",
    "\n",
    "```scala\n",
    "val wiki: RDD[WikiArticle] = ...\n",
    "wiki.map { article=> article.text.toLowerCase } \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So before we get into how to use Spark, how to get good performance out of Spark,\n",
    "and how to express basic analytics jobs in Spark's programming model, let's first\n",
    "look at some of the key ideas behind Spark in an effort to get a bit of intuition\n",
    "about why Spark is causing such a shift in the world of data science and analytics.\n",
    "As we'll see in this section, Spark stands out in how it deals with latency,\n",
    "which is a fundamental concern when a system becomes distributed. \n",
    "\n",
    "__Data-Parallel Programming In the Parallel Programming course, we learned:__\n",
    "- Data parallelism on a single multicore/multi-processor machine.\n",
    "- Parallel collections as an implementation of this paradigm. \n",
    "\n",
    "__Today:__\n",
    "- Data parallelism in a distributed setting.\n",
    "- Distributed collections abstraction from Apache Spark as an implementation of this paradigm.\n",
    "\n",
    "Distribution introduces important concerns beyond what we had to worry about when dealing with parallelism in the shared memory case: \n",
    "\n",
    "1. Partial failure: crash failures of a subset of the machines involved in a distributed computation. \n",
    "2. Latency: certain operations have a much higher latency than other operations due to network communication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory Ops < Disk Ops < Network Comms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big  Data Processing and Latency \n",
    "\n",
    "With some intuition now about how expensive network communication and disk operations can be, one may ask: \n",
    "How do these latency numbers relate to big data processing? \n",
    "\n",
    "To answer this question, let's first start with Spark's predecessor, __Hadoop__. \n",
    "\n",
    "Hadoop is a widely-used large-scale batch data processing framework.  It's an open source implementation of Google's MapReduce. \n",
    "MapReduce was ground-breaking because it provided: \n",
    "- a simple API (simple map and reduce steps) -\n",
    "-  **fault tolerance** \n",
    "\n",
    "Fault tolerance is what made it possible for Hadoop/MapReduce to scale to 100s or 1000s of nodes at all.\n",
    "\n",
    "Hadoop/MapReduce + Fault Tolerance \n",
    "Why is this important? \n",
    "For  100s or  1000s of old commodity machines, likelihood of at least one node failing is very high midway th rough a job. \n",
    "Thus, Hadoop/MapReduce's ability to recover from node failure enabled: computations on unthinkably large data sets to succeed to completion. \n",
    "\n",
    "**Fault tolerance + simple API = At Google,  MapReduce made it possible for an average Google software engineer to craft a   complex pipeline of map/reduce stages on extremely large data sets.**\n",
    "\n",
    "\n",
    "## Why Spark? \n",
    "\n",
    "Fault-tolerance in  Hadoop/MapReduce comes at a cost. Between each map and reduce step, in order to recover from potential failures,  Hadoop/MapReduce shuffles its data and write intermediate data to disk. \n",
    "\n",
    "> Remember: \n",
    "Reading/writing to disk: lOOx slower than in-memory   Network communication: 1,000,000x slower than in-memory \n",
    "\n",
    "Spark \n",
    "\n",
    "- retains fault-tolerance\n",
    "- Different strategy for handling latency (latency significantly reduced!) \n",
    "\n",
    "Spark uses different strategy to reduce latency. Achieves this using ideas from functional programming! \n",
    "> Idea:  Keep all data immutable and in-memory.  All operations on data are just functional transformations, like regular Scala collections.  Fault tolerance is achieved by replaying functional transformations over original dataset.\n",
    "\n",
    "Spark has been shown to be l00x more performant than Hadoop, while adding even more expressive APls. Spark tries to minimize agressively any network traffic and favours in-memory computation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
