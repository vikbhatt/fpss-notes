{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDDs, Spark's Distributed Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDDs seem a lot like immutable sequential or parallel Scala collections. \n",
    "\n",
    "```scala\n",
    "abstract class RDD[T]  { \n",
    "    def map[U](f: T => U): RDD[U] = ... \n",
    "    def flatMap[U](f: T =>  TraversableOnce[U]: RDD[U] = ... \n",
    "    def filter(f: T =>  Boolean): RDD[T] = ... \n",
    "    def reduce(f: (T,  T) =>  T): T = ... \n",
    "```\n",
    "\n",
    "Shown here only few operations.\n",
    "\n",
    "\n",
    "Most operations on RDDs, like Scala's immutable List, and Scala's parallel collections, are higher-order functions. \n",
    "\n",
    "That is,  methods that work on  RDDs, taking a  function as an argument, and which typically return RDDs.\n",
    "\n",
    "While their signatures differ a bit, their semantics (macroscopically) are\n",
    "the same:\n",
    "\n",
    "```scala\n",
    "map[B](f: A=> B): List[B] // Scala List\n",
    "map[B](f: A=> B): RDD[B] // Spark ROD\n",
    "flatMap[B](f: A=> TraversableOnce[B]): List[B] // Scala List\n",
    "flatMap[B](f: A=> TraversableOnce[B]): RDD[B] // Spark ROD\n",
    "filter(pred: A=> Boolean): List[A] // Scala List\n",
    "filter(pred: A=> Boolean): RDD[A] // Spark ROD\n",
    "reduce(op: (A, A)=> A): A// Scala List\n",
    "reduce(op: (A, A)=> A): A// Spark RDD\n",
    "fold(z: A)(op: (A, A)=> A): A// Scala List\n",
    "fold(z: A)(op: (A, A)=> A): A// Spark RDD\n",
    "aggregate[B](z: => B)(seqop: (B, A)=> B, combop: (B, B) => B): B // Scala\n",
    "aggregate[B](z: B)(seqop: (B, A)=> B, combop: (B, B) => B): B // Spark RDD         \n",
    "```\n",
    "\n",
    "Using RDDs in Spark feels a lot like normal Scala sequential/parallel\n",
    "collections, with the added knowledge that your data is distributed across\n",
    "several machines.\n",
    "\n",
    "\n",
    "## How to Create an RDD?\n",
    "RDDs can be created in two ways:\n",
    "- Transforming an existing RDD .\n",
    "- From a SparkContext ( or SparkSession) object.\n",
    "\n",
    "Transforming an existing `RDD`. Just like a call to map on a `List` returns a new `List`, many higher-order functions defined on `RDD` return a new `RDD`. \n",
    "\n",
    "From a `SparkContext` (or `SparkSession`) object .\n",
    "The `SparkContext` object (renamed `SparkSession`) can be thought of as\n",
    "your handle to the Spark cluster. It represents the connection between the\n",
    "Spark cluster and your running application. It defines a handful of methods which can be used to create and populate a new RDD:\n",
    "\n",
    "- `parallelize`: convert a local Scala collection to an RDD .\n",
    "- `textFile`: read a text file from HDFS or a local file system and return an RDD of String "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations and Actions \n",
    "\n",
    "Operations on RDD's. \n",
    "\n",
    "Recall transformers and accessors from Scala sequential and parallel\n",
    "collections.\n",
    "\n",
    "Transformers. Return new collections as results. (Not single values.)\n",
    "Examples: `map`, `filter`, `flatMap`, `groupBy`\n",
    "\n",
    "```scala\n",
    "map(f: A=> B): Traversable[B]\n",
    "```\n",
    "Accessors: Return single values as results. (Not collections.)\n",
    "Examples: `reduce`, `fold`, `aggregate`.\n",
    "```scala\n",
    "reduce(op: (A, A)=> A): A\n",
    "```\n",
    "\n",
    "Transformations: Return new collections RDDs as results. They are lazy, their result RDD is not immediately computed.\n",
    "\n",
    "Actions. Compute a result based on an RDD, and either returned or saved to an external storage system (e.g., HDFS). They are eager, their result is immediately computed.\n",
    "\n",
    "Laziness/eagerness is how we can limit network communication using the programming mode.\n",
    "\n",
    "\n",
    "## Example\n",
    "Consider the following sim ple exam ple:\n",
    "```scala\n",
    "val largelist: List[String] = ...\n",
    "val wordsRdd = sc.parallelize(largelist)\n",
    "val lengthsRdd = wordsRdd.map(_.length)\n",
    "```\n",
    "What has happened on the cluster at this point?\n",
    "Nothing. Execution of map ( a transform at ion) is deferred.\n",
    "To kick off the com putation and wait for its result.\n",
    "\n",
    "\n",
    "\n",
    "<table class=\"table\">\n",
    "<tbody><tr><th style=\"width:25%\">Transformation</th><th>Meaning</th></tr>\n",
    "<tr>\n",
    "  <td> <b>map</b>(<i>func</i>) </td>\n",
    "  <td> Return a new distributed dataset formed by passing each element of the source through a function <i>func</i>. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>filter</b>(<i>func</i>) </td>\n",
    "  <td> Return a new dataset formed by selecting those elements of the source on which <i>func</i> returns true. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>flatMap</b>(<i>func</i>) </td>\n",
    "  <td> Similar to map, but each input item can be mapped to 0 or more output items (so <i>func</i> should return a Seq rather than a single item). </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>mapPartitions</b>(<i>func</i>) </td>\n",
    "  <td> Similar to map, but runs separately on each partition (block) of the RDD, so <i>func</i> must be of type\n",
    "    Iterator[T] =&gt; Iterator[U] when running on an RDD of type T. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>mapPartitionsWithIndex</b>(<i>func</i>) </td>\n",
    "  <td> Similar to mapPartitions, but also provides <i>func</i> with an integer value representing the index of\n",
    "  the partition, so <i>func</i> must be of type (Int, Iterator[T]) =&gt; Iterator[U] when running on an RDD of type T.\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>sample</b>(<i>withReplacement</i>, <i>fraction</i>, <i>seed</i>) </td>\n",
    "  <td> Sample a fraction <i>fraction</i> of the data, with or without replacement, using a given random number generator seed. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>union</b>(<i>otherDataset</i>) </td>\n",
    "  <td> Return a new dataset that contains the union of the elements in the source dataset and the argument. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>distinct</b>([<i>numTasks</i>])) </td>\n",
    "  <td> Return a new dataset that contains the distinct elements of the source dataset.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>groupByKey</b>([<i>numTasks</i>]) </td>\n",
    "  <td> When called on a dataset of (K, V) pairs, returns a dataset of (K, Seq[V]) pairs. <br>\n",
    "<b>Note:</b> By default, this uses only 8 parallel tasks to do the grouping. You can pass an optional <code>numTasks</code> argument to set a different number of tasks.\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>reduceByKey</b>(<i>func</i>, [<i>numTasks</i>]) </td>\n",
    "  <td> When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function. Like in <code>groupByKey</code>, the number of reduce tasks is configurable through an optional second argument. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>sortByKey</b>([<i>ascending</i>], [<i>numTasks</i>]) </td>\n",
    "  <td> When called on a dataset of (K, V) pairs where K implements Ordered, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the boolean <code>ascending</code> argument.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>join</b>(<i>otherDataset</i>, [<i>numTasks</i>]) </td>\n",
    "  <td> When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>cogroup</b>(<i>otherDataset</i>, [<i>numTasks</i>]) </td>\n",
    "  <td> When called on datasets of type (K, V) and (K, W), returns a dataset of (K, Seq[V], Seq[W]) tuples. This operation is also called <code>groupWith</code>. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>cartesian</b>(<i>otherDataset</i>) </td>\n",
    "  <td> When called on datasets of types T and U, returns a dataset of (T, U) pairs (all pairs of elements). </td>\n",
    "</tr>\n",
    "</tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Example\n",
    "\n",
    "Let's assume that we have an `RDD[String]` which contains gigabytes of\n",
    "logs collected over the previous year. Each element of this `RDD` represents\n",
    "one line of logging.\n",
    "Assuming that dates come in the form, `YYYY-MM-DD:HH:MM:SS`, and errors\n",
    "are logged with a prefix that includes the word \"error\"\n",
    "\n",
    "How would you determine the number of errors that were logged in\n",
    "December 2016?\n",
    "```scala\n",
    "val lastYearslogs: RDD[String] = ...\n",
    "val numDecErrorlogs\n",
    "= lastYearslogs.filter(lg => lg.contains(\"2016-12\") && lg.contains(\"error\"))\n",
    ".count()\n",
    "```\n",
    "\n",
    "Spark com putes RDDs the first time they are used in an action.\n",
    "This helps when processing large amounts of data.\n",
    "Example:\n",
    "\n",
    "```scala\n",
    "val lastYearslogs: RDD[String] = ...\n",
    "val firstlogsWithErrors = lastYearslogs.filter(_.contains(\"ERROR\")) .take(10)\n",
    "```\n",
    "The execution of filter is deferred until the take action is applied.\n",
    "Spark leverages this by analyzing and optimizing the chain of operations before\n",
    "executing it.\n",
    "\n",
    "Spark will not compute intermediate RDDs. Instead, as soon as 10 elements of the\n",
    "filtered RDD have been computed, firstLogsWi thErrors is done. At this point Spark\n",
    "stops working, saving time and space computing elements of the unused result of filter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"table\">\n",
    "<tbody><tr><th>Action</th><th>Meaning</th></tr>\n",
    "<tr>\n",
    "  <td> <b>reduce</b>(<i>func</i>) </td>\n",
    "  <td> Aggregate the elements of the dataset using a function <i>func</i> (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>collect</b>() </td>\n",
    "  <td> Return all the elements of the dataset as an array at the driver program. This is usually useful after a filter or other operation that returns a sufficiently small subset of the data. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>count</b>() </td>\n",
    "  <td> Return the number of elements in the dataset. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>first</b>() </td>\n",
    "  <td> Return the first element of the dataset (similar to take(1)). </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>take</b>(<i>n</i>) </td>\n",
    "  <td> Return an array with the first <i>n</i> elements of the dataset. Note that this is currently not executed in parallel. Instead, the driver program computes all the elements. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>takeSample</b>(<i>withReplacement</i>, <i>num</i>, <i>seed</i>) </td>\n",
    "  <td> Return an array with a random sample of <i>num</i> elements of the dataset, with or without replacement, using the given random number generator seed. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>saveAsTextFile</b>(<i>path</i>) </td>\n",
    "  <td> Write the elements of the dataset as a text file (or set of text files) in a given directory in the local filesystem, HDFS or any other Hadoop-supported file system. Spark will call toString on each element to convert it to a line of text in the file. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>saveAsSequenceFile</b>(<i>path</i>) </td>\n",
    "  <td> Write the elements of the dataset as a Hadoop SequenceFile in a given path in the local filesystem, HDFS or any other Hadoop-supported file system. This is only available on RDDs of key-value pairs that either implement Hadoop's Writable interface or are implicitly convertible to Writable (Spark includes conversions for basic types like Int, Double, String, etc). </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>countByKey</b>() </td>\n",
    "  <td> Only available on RDDs of type (K, V). Returns a `Map` of (K, Int) pairs with the count of each key. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>foreach</b>(<i>func</i>) </td>\n",
    "  <td> Run a function <i>func</i> on each element of the dataset. This is usually done for side effects such as updating an accumulator variable (see below) or interacting with external storage systems. </td>\n",
    "</tr>\n",
    "</tbody></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
